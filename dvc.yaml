stages:
  download-images:
    cmd: python scripts/download-images.py datasets/watch-faces.json --concurrent
    deps:
    - scripts/download-images.py
    - datasets/watch-faces.json
    outs:
    - datasets/train:
        persist: true
    - datasets/test:
        persist: true
    - datasets/val:
       persist: true
    - datasets/watch-faces-local.json
  detection-generate-dataset:
    cmd: >-
      python watch_recognition/watch_recognition/datasets/generate_detection_tf_records.py 
      datasets/watch-faces-local.json 
      datasets/tf-records/object-detection/watch-faces/ 
      --num-shards=1
      --run-concurrently
    params:
      - detector.label_to_cls
    deps:
      - datasets/train
      - datasets/test
      - datasets/val
      - datasets/watch-faces-local.json
    outs:
      - datasets/tf-records/object-detection/watch-faces/watch-faces-test-00001-of-00001.tfrecord
      - datasets/tf-records/object-detection/watch-faces/watch-faces-train-00001-of-00001.tfrecord
      - datasets/tf-records/object-detection/watch-faces/watch-faces-val-00001-of-00001.tfrecord
  detection-train:
    cmd: >- 
      python watch_recognition/watch_recognition/train/object_detection_tf_model_garden.py
      "train-configs/tf-model-garden/watch-face-detector/"
      --seed ${seed}
    params:
      - detector
      - seed
    deps:
      - datasets/tf-records/object-detection/watch-faces/
      - train-configs/tf-model-garden/watch-face-detector/retinanet_task.yaml
      - train-configs/tf-model-garden/watch-face-detector/runtime.yaml
      - train-configs/tf-model-garden/watch-face-detector/trainer.yaml
    outs:
      - debug/detector/:
          persist: true
      - models/detector/:
          persist: false
  detection-export:
    cmd: >-
      python watch_recognition/watch_recognition/export/export_retinanet.py models/detector/
    deps:
      - models/detector/
      - datasets/tf-records/object-detection/watch-faces/watch-faces-train-00001-of-00001.tfrecord
      - train-configs/tf-model-garden/watch-face-detector/retinanet_task.yaml
      - train-configs/tf-model-garden/watch-face-detector/runtime.yaml
      - train-configs/tf-model-garden/watch-face-detector/trainer.yaml
    outs:
      - exported_models/detector/lite16/assets.extra/tf_serving_warmup_requests:
          persist: true
      - exported_models/detector/lite16/model.tflite:
          persist: true
      - exported_models/detector/lite8/assets.extra/tf_serving_warmup_requests:
          persist: true
      - exported_models/detector/lite8/model.tflite:
          persist: true
      - exported_models/detector/serving/assets.extra/tf_serving_warmup_requests:
          persist: true
      - exported_models/detector/serving/variables/variables.data-00000-of-00001:
          persist: true
      - exported_models/detector/serving/variables/variables.index:
          persist: true
      - exported_models/detector/serving/fingerprint.pb:
          persist: true
      - exported_models/detector/serving/saved_model.pb:
          persist: true
    plots:
      - example_predictions/detector/IMG_0040.jpg
  detection-eval:
    cmd: python watch_recognition/watch_recognition/eval/object_detection_eval.py
    deps:
      - exported_models/detector/serving/
      - datasets/train
      - datasets/test
      - datasets/val
    metrics:
      - metrics/detector/train/coco.json:
          cache: false
      - metrics/detector/val/coco.json:
          cache: false
      - metrics/detector/train/detection.json:
          cache: false
      - metrics/detector/val/detection.json:
          cache: false
    plots:
      - metrics/detector/train/PR-IoU@0.50.tsv:
          x: Recall
          y: Precision
          title: Train PR-IoU@0.50
          cache: false
      - metrics/detector/train/PR-IoU@0.75.tsv:
          x: Recall
          y: Precision
          title: Train PR-IoU@0.75
          cache: false
      - metrics/detector/train/PR-IoU@0.95.tsv:
          x: Recall
          y: Precision
          title: Train PR-IoU@0.95
          cache: false
      - metrics/detector/val/PR-IoU@0.50.tsv:
          x: Recall
          y: Precision
          title: Val PR-IoU@0.50
          cache: false
      - metrics/detector/val/PR-IoU@0.75.tsv:
          x: Recall
          y: Precision
          title: Val PR-IoU@0.75
          cache: false
      - metrics/detector/val/PR-IoU@0.95.tsv:
          x: Recall
          y: Precision
          title: Val PR-IoU@0.95
          cache: false
      - example_predictions/detector/train_0.jpg
      - example_predictions/detector/train_1.jpg
      - example_predictions/detector/train_2.jpg
      - example_predictions/detector/val_0.jpg
      - example_predictions/detector/val_1.jpg
      - example_predictions/detector/val_2.jpg
  keypoint-train:
    cmd: >-
      python watch_recognition/watch_recognition/train/heatmap_regression_task.py
      --epochs ${keypoint.epochs}
      --batch-size ${keypoint.batch-size}
      --confidence-threshold ${keypoint.confidence-threshold}
      --seed ${seed}
    params:
      - keypoint
      - seed
      - max_images
    deps:
      - datasets/train
      - datasets/test
      - datasets/val
      - datasets/watch-faces-local.json
      - watch_recognition/watch_recognition/train/heatmap_regression_task.py
    metrics:
      - metrics/keypoint/metrics.json:
          cache: false
    plots:
      - metrics/keypoint/plots/:
          cache: false
      - example_predictions/keypoint/test-image-2.jpg:
          persist: true
    outs:
      - models/keypoint/:
          persist: true
      - debug/keypoint/:
          persist: true
  keypoint-eval:
    cmd: python watch_recognition/watch_recognition/eval/keypoint_detection_eval.py --kp-confidence-threshold ${keypoint.confidence-threshold}
    deps:
      - models/keypoint/
      - models/detector/
      - watch_recognition/watch_recognition/eval/keypoint_detection_eval.py
    metrics:
      - metrics/keypoint/coco_train.json:
          cache: false
      - metrics/keypoint/coco.json:
          cache: false
    plots:
      - example_predictions/keypoint/train_0.jpg
      - example_predictions/keypoint/train_1.jpg
      - example_predictions/keypoint/train_2.jpg
      - example_predictions/keypoint/train_3.jpg
      - example_predictions/keypoint/train_4.jpg
      - example_predictions/keypoint/val_0.jpg
      - example_predictions/keypoint/val_1.jpg
      - example_predictions/keypoint/val_2.jpg
      - example_predictions/keypoint/val_3.jpg
      - example_predictions/keypoint/val_4.jpg
  segmentation-generate-dataset:
    cmd: >-
      python watch_recognition/watch_recognition/datasets/generate_segmentation_tf_records.py
      datasets/watch-faces-local.json 
      datasets/tf-records/segmentation/watch-hands/ 
      --num-shards=1
      --run-concurrently
    params:
      - segmentation.label_to_cls
      - segmentation.bbox_labels
    deps:
      - datasets/test
      - datasets/val
      - datasets/watch-faces-local.json
    outs:
      - datasets/tf-records/segmentation/watch-hands/watch-hands-train-00001-of-00001.tfrecord
      - datasets/tf-records/segmentation/watch-hands/watch-hands-val-00001-of-00001.tfrecord
  segmentation-train:
    cmd: >-
      python watch_recognition/watch_recognition/train/segmentation_tf_model_garden.py
      --seed ${seed}
    params:
      - segmentation
      - seed
    deps:
      - datasets/tf-records/segmentation/watch-hands/watch-hands-train-00001-of-00001.tfrecord
      - datasets/tf-records/segmentation/watch-hands/watch-hands-val-00001-of-00001.tfrecord
      - train-configs/tf-model-garden/watch-hands-segmentation
      - checkpoints/segmentation/deeplabv3_mobilenetv2_coco
    plots:
      - debug/segmentation/train_data_0.png:
          persist: true
      - debug/segmentation/train_data_1.png:
          persist: true
      - debug/segmentation/train_data_2.png:
          persist: true
      - debug/segmentation/train_data_predict_0.png:
          persist: true
    outs:
      - models/segmentation/:
          persist: false
  segmentation-export:
    cmd: >-
      python watch_recognition/watch_recognition/export/export_segmentation.py models/segmentation train-configs/tf-model-garden/watch-hands-segmentation/
    deps:
      - models/segmentation/
      - train-configs/tf-model-garden/watch-hands-segmentation
    plots:
      - example_predictions/segmentation/hands/example-1.jpg:
          persist: true
    outs:
      - exported_models/segmentation/hands/:
          persist: true
  segmentation-eval:
    cmd: python watch_recognition/watch_recognition/eval/segmentation_eval.py --confidence-threshold ${segmentation.confidence-threshold} --save-per-image-metrics
    deps:
      - exported_models/segmentation/hands
      - watch_recognition/watch_recognition/eval/segmentation_eval.py
    plots:
      - example_predictions/segmentation/hands/train_0.jpg
      - example_predictions/segmentation/hands/train_1.jpg
      - example_predictions/segmentation/hands/train_2.jpg
      - example_predictions/segmentation/hands/train_3.jpg
      - example_predictions/segmentation/hands/train_4.jpg
      - example_predictions/segmentation/hands/val_0.jpg
      - example_predictions/segmentation/hands/val_1.jpg
      - example_predictions/segmentation/hands/val_2.jpg
      - example_predictions/segmentation/hands/val_3.jpg
      - example_predictions/segmentation/hands/val_4.jpg
    outs:
      - metrics/segmentation/hands/train/segmentation_eval_per_image.csv
      - metrics/segmentation/hands/val/segmentation_eval_per_image.csv
    metrics:
      - metrics/segmentation/hands/train/segmentation.json
      - metrics/segmentation/hands/val/segmentation.json
  end-2-end-eval:
    cmd: python watch_recognition/watch_recognition/eval/end_to_end_eval.py --run-concurrently --split=val
    deps:
      - watch_recognition/watch_recognition/eval/end_to_end_eval.py
      - models/detector
      - models/keypoint
      - models/segmentation
      - datasets/watch-faces-local.json

    metrics:
      - metrics/end_2_end_eval.csv
      - metrics/end_2_end_summary.json
  update-metrics:
      cmd: python scripts/update-metrics-table-and-graph.py
      deps:
        - scripts/update-metrics-table-and-graph.py
        - metrics
  render-demo:
      cmd: scripts/render-demo-movie.py example_data/IMG_1200_720p.mov demo/demo.mp4 --enable-multithreading
      deps:
        - scripts/render-demo-movie.py
        - example_data/IMG_1200_720p.mov
        - exported_models/segmentation/hands
        - exported_models/detector
        - models/keypoint
      outs:
        - demo/demo.mp4
